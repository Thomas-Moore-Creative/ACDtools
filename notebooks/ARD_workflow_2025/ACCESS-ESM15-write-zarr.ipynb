{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa886a34-4486-4bb3-9a67-bcc07a80d3e1",
   "metadata": {},
   "source": [
    "# ðŸ““ ACCESS-ESM15-write-zarr\n",
    "\n",
    "**Author:** Thomas Moore  \n",
    "**Date:** 2025-10-31\n",
    "**Updated:** YYYY-MM-DD (if applicable)  \n",
    "**Environment:** `pangeo_csepta` running on `Gadi` ARE  \n",
    "**Tags:** sandbox, ARD, ACCESS-ESM1.5\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“˜ Description\n",
    "\n",
    "This notebook attempts to bring together all the disperate workplows for this task over the year(s).  Issue: [https://github.com/Thomas-Moore-Creative/Pacific-Tuna-Climate-Response/issues/51](https://github.com/Thomas-Moore-Creative/Pacific-Tuna-Climate-Response/issues/51) and sub-issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c1e700b-839d-479b-9c90-31caeadfd630",
   "metadata": {},
   "outputs": [],
   "source": [
    "Author = {\"name\": \"Thomas Moore\", \"affiliation\": \"CSIRO\", \"email\": \"thomas.moore@csiro.au\", \"orcid\": \"0000-0003-3930-1946\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cccb7e8-462f-49d7-b0d8-4bc0f3708308",
   "metadata": {},
   "source": [
    "# Software"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56670eb-b4a8-4197-8440-dc3fe30fa185",
   "metadata": {},
   "source": [
    "### ACDtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d9f9e65-9567-4c17-a16f-e552eac42c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///g/data/es60/users/thomas_moore/code/ACDtools\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: ACDtools\n",
      "  Building editable for ACDtools (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ACDtools: filename=acdtools-0.1-0.editable-py3-none-any.whl size=3553 sha256=3ad6270b8f5325bc968cd37bcba9d5374b414a6f9573f887e869d50483e51336\n",
      "  Stored in directory: /scratch/es60/thomas_moore/tmp/pip-ephem-wheel-cache-giwui4in/wheels/b6/a3/f2/6ce45fbdc116ad50e421d6a11cb060cc796e867501807af446\n",
      "Successfully built ACDtools\n",
      "Installing collected packages: ACDtools\n",
      "  Attempting uninstall: ACDtools\n",
      "    Found existing installation: ACDtools 0.1\n",
      "    Uninstalling ACDtools-0.1:\n",
      "      Successfully uninstalled ACDtools-0.1\n",
      "Successfully installed ACDtools-0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --user -e /g/data/es60/users/thomas_moore/code/ACDtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e01a158-bbb3-454f-8a35-1c575129d1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autoreload in the notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 1 \n",
    "%aimport ACDtools.util\n",
    "%aimport ACDtools.ard\n",
    "%aimport ACDtools.ocean\n",
    "%aimport ACDtools.plot\n",
    "# Importing from your local package util.py\n",
    "from ACDtools import util\n",
    "from ACDtools import ard\n",
    "from ACDtools import ocean\n",
    "from ACDtools import plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d07c7db5-700e-4ce4-a533-9c497a0c13d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pprint import pprint\n",
    "import intake\n",
    "import sys\n",
    "from contextlib import redirect_stdout\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d198700-c45d-4fd3-9967-2af4fabf26a4",
   "metadata": {},
   "source": [
    "## Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ab80f62-ecc8-4233-b25e-f8ae57847ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster started with 28 workers.\n",
      "Dashboard available at: /proxy/8787/status\n"
     ]
    }
   ],
   "source": [
    "client, cluster = util.start_dask_cluster_from_config('netcdf_work')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8307cc-4980-42dd-91a4-e19de538c670",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27356386-d239-47ee-b9e9-67bfd2641049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# develop any new functions here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f3fd84-6bc3-436f-8c15-2bc5ac723805",
   "metadata": {},
   "source": [
    "# settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab524f37-1a06-4c1a-98b1-22b4e26abc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config_path = '/g/data/es60/users/thomas_moore/code/ACDtools/job_config.yaml'\n",
    "job_config_dict = ACDtools.util.load_config(job_config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65dda70-c5c0-45f2-8523-18b7f6a923f6",
   "metadata": {},
   "source": [
    "# workflow with ACCESS-NRI catalog that uses NCI catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fa2c85d-89a9-460c-86e9-b0f4ffae1446",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = intake.cat.access_nri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31bca559-b5b5-42a6-86ba-5271c55f7456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Log written to /scratch/es60/ard/models/ACCESS-ESM15/ARD/logs/log_20251106_152528.txt\n",
      "CPU times: user 2min 9s, sys: 16.1 s, total: 2min 25s\n",
      "Wall time: 2min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- build log filename dynamically ---\n",
    "logfile = f\"{job_config_dict['paths']['log_dir']}log_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "\n",
    "with open(logfile, \"w\") as f, redirect_stdout(f):\n",
    "    #####\n",
    "    print('Running: ' +\n",
    "          job_config_dict['catalog_search_query_dict']['source_id'] +\n",
    "          ' ' + job_config_dict['catalog_search_query_dict']['experiment_id'] +\n",
    "          ' ' + job_config_dict['catalog_search_query_dict']['variable_id'] +\n",
    "          ' ' + job_config_dict['catalog_search_query_dict']['table_id'] +\n",
    "          ' ' + job_config_dict['catalog_search_query_dict']['version']\n",
    "         )\n",
    "    pprint(job_config_dict)\n",
    "    # load catalog\n",
    "    cmip6_fs38_datastore = ACDtools.util.load_cmip6_fs38_datastore()\n",
    "    # search catalog for list of files\n",
    "    search = cmip6_fs38_datastore.search(**job_config_dict['catalog_search_query_dict'])\n",
    "    # load into one object using xarray kwags for chunking and handling cftime\n",
    "    ####\n",
    "    #ds = load_ACCESS_ESM(search,use_cftime=True,chunking_key=job_config_dict['chunking_key'])\n",
    "    ####\n",
    "    ds = ACDtools.ard.load_ACCESS_ESM_ensemble(search,use_cftime=True,chunking_key=job_config_dict['chunking_key'])\n",
    "    # save and drop multidimensional coordinates\n",
    "    ds = ACDtools.ard.save_n_drop_multidim_lat_lon(ds,save_coords_dir=job_config_dict['paths']['save_coords_dir'],\n",
    "                                      variable_name = job_config_dict['catalog_search_query_dict']['variable_id'])\n",
    "    # remove encoding\n",
    "    ACDtools.util.remove_encoding(ds)\n",
    "    # write out zarr\n",
    "    current_datetime = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    print('Started zarr write at '+current_datetime)\n",
    "    filename = (\n",
    "    f\"{job_config_dict['paths']['write_dir']}\"\n",
    "    + f\"{job_config_dict['catalog_search_query_dict']['source_id']}.\"\n",
    "    + f\"{job_config_dict['catalog_search_query_dict']['experiment_id']}.\"\n",
    "    + f\"{job_config_dict['catalog_search_query_dict']['variable_id']}.\"\n",
    "    + f\"base.v{current_datetime}.zarr\"\n",
    "    )\n",
    "    print('Zarr filename = '+filename )\n",
    "    ds.to_zarr(filename,consolidated=True)\n",
    "    print('Finished at '+ current_datetime)\n",
    "    ##### start log cluster settings\n",
    "    # Get existing client (do NOT create a new one)\n",
    "    client = client.current()   # or Client() if not yet connected\n",
    "\n",
    "    info = client.scheduler_info()[\"workers\"]\n",
    "\n",
    "    # total cores (threads)\n",
    "    n_workers = len(info)\n",
    "    total_threads = sum(w.get(\"nthreads\", 0) for w in info.values())\n",
    "\n",
    "    # helper: MiB/GB conversion using powers of 1024\n",
    "    def to_gib(bytes_):\n",
    "        return bytes_ / (1024 ** 3)\n",
    "\n",
    "    # 1) Try configured memory_limit\n",
    "    limits = {addr: w.get(\"memory_limit\", 0) for addr, w in info.items()}\n",
    "\n",
    "    # 2) For any worker reporting 0/None (unlimited), fetch actual host RAM via psutil on the worker\n",
    "    need_actual = [addr for addr, lim in limits.items() if not lim or lim == 0 or math.isclose(lim, 0.0)]\n",
    "    \n",
    "    if need_actual:\n",
    "        # run a tiny function on workers to read psutil.virtual_memory().total\n",
    "        def _get_total_ram():\n",
    "            try:\n",
    "                import psutil\n",
    "                return psutil.virtual_memory().total\n",
    "            except Exception:\n",
    "                # fallback: dask.system.MEMORY_LIMIT may help on some installs\n",
    "                try:\n",
    "                    from dask.system import MEMORY_LIMIT\n",
    "                    return MEMORY_LIMIT or 0\n",
    "                except Exception:\n",
    "                    return 0\n",
    "    \n",
    "        actual = client.run(_get_total_ram)  # returns {worker_addr: bytes}\n",
    "        # fill in missing/zero limits with actual system RAM\n",
    "        for addr in need_actual:\n",
    "            if addr in actual and actual[addr]:\n",
    "                limits[addr] = actual[addr]\n",
    "    \n",
    "    total_mem_bytes = sum(limits.values())\n",
    "    total_mem_gib = to_gib(total_mem_bytes)/n_workers\n",
    "    \n",
    "    # Build a short log block\n",
    "    log_lines = [\n",
    "        \"Dask cluster resources:\",\n",
    "        f\"  Workers:      {n_workers}\",\n",
    "        f\"  Total cores:  {total_threads}\",\n",
    "        f\"  Total memory: {total_mem_gib:.2f} GiB\",\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\".join(log_lines))\n",
    "    ########### end log cluster settings\n",
    "    print(\"==== Log complete ====\")\n",
    "# ---------------------------------------------------------\n",
    "print(f\"âœ… Log written to {logfile}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efb307e-b133-4d84-8127-d3a1fd57e97a",
   "metadata": {},
   "source": [
    "# THE END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068f34d6-c43d-4e7e-bce0-5883d121cd6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
